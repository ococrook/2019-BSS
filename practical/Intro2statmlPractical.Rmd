---
title: "Introduction to Statistics and Machine Learning"
author: "Oliver M. Crook"
date: "27 June 2019"
output: html_document
---

Many of these example have been taken and adapted from 

Modern Statistics for Modern Biology; Susan Holmes and Wolfgang Huber

An Introduction to Statistical Learning; James, Witten, Hastie, Tibshirnani

The Elements to Statistical Learning; Friedman, Tibshirani, Hastie

# Section: Testing

## Is my coin biased?

The following code chunk simualtes 10 coin toss from a biased coin.

```{r,}
set.seed(2) # sets a random seed
numFlips = 100
probHead = 0.59
coinFlips = sample(c("H", "T"), size = numFlips,
  replace = TRUE, prob = c(probHead, 1 - probHead))
head(coinFlips)

```
If the coin were unbiased we expect roughly 50 heads. Let us see how many heads and tails there are.
```{r,}
table(coinFlips)
```

We calculate the binomial statistic for a number of flips between 0 and 100. This is the binomial density
for an unbiased coin.
```{r,}
library("dplyr")
k = 0:numFlips
numHeads = sum(coinFlips == "H")
binomDensity = tibble(k = k,
     p = dbinom(k, size = numFlips, prob = 0.5))
```
Questions: 
1) Write some code to check that the probabilties from the binomial statistic sum to $1$.

2) change the prob argument show that the probabilities still some to $1$.


The following code chunk plots the binomial statistic and the number of heads observed is marked in blue.

```{r,}
library("ggplot2")
ggplot(binomDensity) +
  geom_bar(aes(x = k, y = p), stat = "identity") +
  geom_vline(xintercept = numHeads, col = "blue")

```

Question:

3) Change the prob argument above and replot the binomial statistic, what do you notice about how the distribution is centered?


Now, we set the size of the reject threshold
```{r,}
alpha = 0.05
```

Question:

4) Without looking below use the arrange function from dplyr to order the probabilities, smallest first.

5) Looking at the output, what is the most unlikely number of heads to observe?

6) Looking at the output, what is the most likely number of heads to observe?



Solution:
```{r,}
orderBinomDenisty <- arrange(binomDensity, p)
orderBinomDenisty[1,] # most unlikely
orderBinomDenisty[101,] 
```
Question:

7) What does the following code chunk do?

```{r,}
binomDensity =  arrange(binomDensity, p) %>%
        mutate(reject = (cumsum(p) <= alpha))

```


Let us plot the reject region in red
```{r,}
ggplot(binomDensity) +
  geom_bar(aes(x = k, y = p, fill = reject), stat = "identity") +
  scale_fill_manual(
    values = c(`TRUE` = "red", `FALSE` = "darkgrey")) + theme_minimal() + 
  geom_vline(xintercept = numHeads, col = "blue") +
  theme(legend.position = "none")


```

Questions:

8) Is there evidence that are coin is biased?

9) Change the size of the reject region to a smaller value, what is our conclusion now?


The above test already has an easy to use funciton in R:
```{r,}
binom.test(x = numHeads, n = numFlips, p = 0.5)

```

## The t-test

The following is some plant growth data with 2 treatments. The alpha parameter below
has nothing to do with the reject region before. It changes the see-throughness of the
points to better visualise overlapping points. Have a go at changing it.

```{r,}
library("ggbeeswarm")
data("PlantGrowth")
ggplot(PlantGrowth, aes(y = weight, x = group, col = group, size = 4, alpha = 0.7)) +
  geom_beeswarm() + theme(legend.position = "none") + theme_minimal()
```

Let us take a look at the PlantGrowth data
```{r,}
head(PlantGrowth)
```

Question: 

10) Using a t-test, check to see whether the difference in means of the ctrl and treatment 1 are different


Solution
```{r,}
t.test(PlantGrowth$weight[PlantGrowth$group == "ctrl"], PlantGrowth$weight[PlantGrowth$group == "trt1"], var.equal = TRUE)
```
Questions:

11) What is the p-value?

12) Check the degrees of freedom is correct from the formula?

13) How do the assumptions of the t-test match up with the data?

14) Check the arguments of the t.test function? What happens if you change the equal variance assumption?

You may find the following code useful.

```{r,}
tt = with(PlantGrowth,
          t.test(weight[group =="ctrl"],
                 weight[group =="trt2"],
                 var.equal = TRUE))
tt
```

### Optional: Permtutation test example
We briefly talked about permutation tests in the lecture. Here is an example of a permutation
test. We determine a null distribution of the statistic by random permutation of the group labels.

```{r, cache = TRUE}
abs_t_null = with(
  dplyr::filter(PlantGrowth, group %in% c("ctrl", "trt2")),
    replicate(10000,
      abs(t.test(weight ~ sample(group))$statistic)))
```

Question

15) Why did we use the absolute value function (abs) in the above code?


```{r,}
ggplot(tibble(`|t|` = abs_t_null), aes(x = `|t|`)) +
  geom_histogram(binwidth = 0.1, boundary = 0) + theme_minimal() + 
  geom_vline(xintercept = abs(tt$statistic), col = "red")
```

The following computes the p-value.

```{r,}
mean(abs(tt$statistic) <= abs_t_null)

```
Question

16) Check by writing some code that this value is the same as the rank of the t statistic divided by the number of permutations.

17) Compare the p-value with the one computed using the t-test


# Linear Models

This section discusses linear models. There are many different software packages to work with in R, we will start with some simple examples.


Load libraries require for this section.
```{r,}
library(MASS)
library(ISLR)
```
We load the boston housing market dataset, which includes data on the house price
values in different neighbourhoods in Boston

```{r,}
data(Boston) # use fix(Boston) to bring up a data editor 
names(Boston)
```
In R, the formula syntax is the following `lm(y~x,data)`. The first part specifies 
the formula of the model (an intercept is included by default) and then the variables
to regress on. We create a simple linear model with the median values as a reponse
and the percent of households with low socioeconomic status (lstat) as a predictor.

```{r,}
lm.fit <- lm(medv ~ lstat , data = Boston) # You need to tell R what data to use
```

The output can be looked at using the summary function. Note how t statistics, R^2 and F statistics,
as well as p-values are all outputed automatically. This makes it very easy to apply all the theory
we learnt in lectures.

```{r,}
summary(lm.fit)
coef(lm.fit) # extract coefficients
confint(lm.fit) # extract confidence intervals
```
Question

18) Take the coefficients and calculate the statistics manually (using code not paper!)

We can predict for new values of `lstat`, along with confidence intervals using `predict`
```{r,}
predict (lm.fit ,data.frame(lstat=(c(5,10 ,15))), interval = "confidence")
```

Let us visualise the results of the linear modelling. What is happening?

```{r,}
plot(Boston$lstat, Boston$mdev, pch = 19)
abline (lm.fit ,lwd=3,col ="red")
```

Plotting the lm object results in a diagnostic plots.

```{r,}
par(mfrow=c(2,2))
plot(lm.fit)
```

## Multiple regression

The synatax `lm(y~x1+x2+x3)` is used for multiple linear regression where we
are regressing on predictors `x1`, `x2`, `x3` etc. The following example uses lstat
and age to fit a model

```{r,}
lm.fit <- lm(medv ~ lstat + age , data = Boston )
summary(lm.fit)

```

Question

19) Fit at least 5 different linear models using the above synatax? What do you find out?


If we want to use all predictors in the data frame we just put a point, as follows.

```{r,}
lm.fit <- lm(medv~., data = Boston)
summary (lm.fit)
```

A minus sign can be use to indicate NOT to use that predictor.
```{r,}
lm.fit <- lm(medv ~.-age, data = Boston)
```


